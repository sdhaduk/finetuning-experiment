{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157ac37a-2337-4f37-a250-3325465e0229",
   "metadata": {},
   "source": [
    "# Evaluating Each Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea3273e-5249-4d7a-b6f0-2224a9279c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, logging\n",
    "from peft import LoraConfig, PeftModel\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "from torch import autocast\n",
    "import json\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "output_dir = \"./results\"\n",
    "logging_dir = \"./logs\"\n",
    "\n",
    "model_one_name = \"google/gemma-2b-it\" # already instruction fine-tuned\n",
    "model_two_name = \"microsoft/phi-2\" # focused  on code, chat and QA tasks\n",
    "model_three_name = \"mistralai/Mistral-7B-v0.1\" # only pretrained, but much larger (7B params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4913e1-5442-4b41-9d04-b5bd2c2ef28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'What fraction has parentheses of 0.(3)?', 'input': 'CREATE TABLE table_36623 (\\n    \"Fraction\" text,\\n    \"Ellipsis\" text,\\n    \"Vinculum\" text,\\n    \"Dots\" text,\\n    \"Parentheses\" text\\n)', 'response': 'SELECT \"Fraction\" FROM table_36623 WHERE \"Parentheses\" = \\'0.(3)\\'', 'source': 'wikisql', 'text': 'Below are sql tables schemas paired with instruction that describes a task. Using valid SQLite, write a response that appropriately completes the request for the provided tables. ### Instruction: What fraction has parentheses of 0.(3)? ### Input: CREATE TABLE table_36623 (\\n    \"Fraction\" text,\\n    \"Ellipsis\" text,\\n    \"Vinculum\" text,\\n    \"Dots\" text,\\n    \"Parentheses\" text\\n) ### Response: SELECT \"Fraction\" FROM table_36623 WHERE \"Parentheses\" = \\'0.(3)\\''}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Clinton/Text-to-sql-v1\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "dataset = dataset.select(range(25000))\n",
    "test_set = dataset.select(range(20000, 20010))\n",
    "\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72452ed1-3c59-4ef9-bfd0-07e9dd1b8748",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9e605f-c5a4-4373-b0d5-9d90a3bc6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_gemma(instruction, input_text):\n",
    "    return (\n",
    "        f\"<start_of_turn>user\\n{instruction}\\n\\n{input_text}\\n<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model\\n\"\n",
    "    )\n",
    "\n",
    "def format_prompt_phi2(instruction, input_text):\n",
    "    return (\n",
    "        f\"Instruct: {instruction}\\n\"\n",
    "        f\"Input: {input_text}\\n\"\n",
    "        f\"Output:\"\n",
    "    )\n",
    "\n",
    "def format_prompt_mistral(instruction, input_text):\n",
    "    return (\n",
    "        f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "        f\"### Input:\\n{input_text}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "def get_model_prompt(instruction, input_text, model_name):\n",
    "    if model_name == \"gemma\":\n",
    "        return format_prompt_gemma(instruction, input_text)\n",
    "    elif model_name == \"phi2\":\n",
    "        return format_prompt_phi2(instruction, input_text)\n",
    "    elif model_name == \"mistral\":\n",
    "        return format_prompt_mistral(instruction, input_text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b783237f-85af-465b-b80b-ed180205dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_response(full_output, model_name):\n",
    "    if model_name == \"gemma\":\n",
    "        # Look for response after \"<start_of_turn>model\"\n",
    "        if \"model\" in full_output:\n",
    "            response = full_output.split(\"model\", 1)[1].strip()\n",
    "        else:\n",
    "            response = full_output.strip()\n",
    "\n",
    "        # Optional: strip any <end_of_turn> after the response\n",
    "        response = response.replace(\"<end_of_turn>\", \"\").strip()\n",
    "\n",
    "    elif model_name == \"phi2\":\n",
    "        # Expecting: \"Output: [response]\"\n",
    "        if \"Output:\" in full_output:\n",
    "            response = full_output.split(\"Output:\", 1)[1].strip()\n",
    "        else:\n",
    "            response = full_output.strip()\n",
    "\n",
    "    elif model_name == \"mistral\":\n",
    "        # Expecting: \"### Response:\\n[response]\"\n",
    "        if \"### Response:\" in full_output:\n",
    "            response = full_output.split(\"### Response:\", 1)[1].strip()\n",
    "        else:\n",
    "            response = full_output.strip()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea1b74b-9607-4d47-ba71-61aed0b638bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_model_responses(test_data, model, tokenizer, model_name, response_key, max_tokens=100):\n",
    "    model_responses = {}\n",
    "    for i, example in enumerate(test_data):\n",
    "        print(f\"Example {i+1}\")\n",
    "        instruction = example[\"instruction\"]\n",
    "        input_text = example[\"input\"]\n",
    "\n",
    "        # Format prompt\n",
    "        prompt = get_model_prompt(instruction, input_text, model_name)\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.bfloat16):  # or use device_type=\"cuda\" for PyTorch >= 2\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "        # Decode full output\n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract response depending on model format\n",
    "        generated_response = extract_model_response(full_output, model_name)\n",
    "\n",
    "        # Attach to example\n",
    "        model_responses[i] = generated_response\n",
    "        \n",
    "    return model_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a133ac2d-45ce-4579-a9fe-2a423c807c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_path = f\"{output_dir}/{model_one_name}\"\n",
    "phi_path = f\"{output_dir}/{model_two_name}\"\n",
    "mistral_path = f\"{output_dir}/{model_three_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae93b26-fd97-4912-8074-4a74db167acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd51f07-c46c-41cc-ad32-e8f1998b3ef7",
   "metadata": {},
   "source": [
    "## Gemma Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "518cc078-292d-4947-870a-878582d41599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f8f6c8257348f2acb36728ba93a077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gemma_path,\n",
    "    token=hf_token,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(model_one_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c3a15e-7db2-47b0-9e95-ebb32cedd91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Example 2\n",
      "Example 3\n",
      "Example 4\n",
      "Example 5\n",
      "Example 6\n",
      "Example 7\n",
      "Example 8\n",
      "Example 9\n",
      "Example 10\n"
     ]
    }
   ],
   "source": [
    "gemma_responses = attach_model_responses(test_set, gemma_model, gemma_tokenizer, \"gemma\", \"gemma_response\")\n",
    "with open(\"gemma_responses.json\", \"w\") as f:\n",
    "    json.dump(gemma_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49b0b09f-6738-456a-a901-295384171a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"Fraction\" FROM table_36623 WHERE \"Parentheses\" = '0.(3)'\n"
     ]
    }
   ],
   "source": [
    "print(gemma_responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4f785-e012-4033-8a68-f158eeebfb29",
   "metadata": {},
   "source": [
    "## Phi-2 Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc0b402-bfc3-42e7-b943-eb53aebe54b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82deed28c054e098e8f308b4e93b737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_path,\n",
    "    token=hf_token,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(model_two_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5dbc9ed-9952-46bc-b429-8c999b5cf6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Example 2\n",
      "Example 3\n",
      "Example 4\n",
      "Example 5\n",
      "Example 6\n",
      "Example 7\n",
      "Example 8\n",
      "Example 9\n",
      "Example 10\n"
     ]
    }
   ],
   "source": [
    "phi_responses = attach_model_responses(test_set, phi_model, phi_tokenizer, \"phi2\", \"phi2_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "320a0fca-dd23-4747-8a8e-029991dcac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"phi_responses.json\", \"w\") as f:\n",
    "    json.dump(phi_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8851c4bb-4a05-4db9-bbf4-fbba81f08870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"Fraction\" FROM table_36623 WHERE \"Parentheses\" = '0.(3)' AND \"Ellipsis\" = '\\u00a3\\u00a3\\u00a3\\u00a3\\u00a3\\u00a3\\u00a3' AND \"Dots\" = '\\u00a3\\u00a3\\u00a3\\u00a3\\u00a3\\u00\n"
     ]
    }
   ],
   "source": [
    "print(phi_responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f74fe-21ea-42de-97b4-77c0747d382e",
   "metadata": {},
   "source": [
    "## Mistral Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "059404de-ddc8-4e9d-908d-1c044df81a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2171835b431245fa85353674fa41253f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_path,\n",
    "    token=hf_token,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(model_three_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e15820-b54a-4688-bc2a-6fc3fd888232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Example 2\n",
      "Example 3\n",
      "Example 4\n",
      "Example 5\n",
      "Example 6\n",
      "Example 7\n",
      "Example 8\n",
      "Example 9\n",
      "Example 10\n"
     ]
    }
   ],
   "source": [
    "mistral_responses = attach_model_responses(test_set, mistral_model, mistral_tokenizer, \"mistral\", \"mistral_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8d0c444-770e-4abf-b69d-c225a79ce028",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mistral_responses.json\", \"w\") as f:\n",
    "    json.dump(mistral_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d0591f0-7d8b-4d10-9b1b-06d377650dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"Fraction\" FROM table_36623 WHERE \"Parentheses\" = '0.(3)'\n"
     ]
    }
   ],
   "source": [
    "print(mistral_responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1991202-9d39-41ec-aa04-634926db6574",
   "metadata": {},
   "source": [
    "# Evaluating Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b4fd89c-a82e-4458-a4f6-caccdec0629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(entry, model_response):\n",
    "    return (\n",
    "        f\"Instruction:\\n{entry['instruction']}\\n\\n\"\n",
    "        f\"Input:\\n{entry['input']}\\n\\n\"\n",
    "        f\"Correct Answer:\\n{entry['response']}\\n\\n\"\n",
    "        f\"Model's Answer:\\n{model_response}\\n\\n\"\n",
    "        f\"Score the model's answer from 0 to 100 based on how correct and complete it is. \"\n",
    "        f\"Only respond with the score as an integer (no other text).\"\n",
    "    )\n",
    "\n",
    "\n",
    "import urllib.request as req    \n",
    "def query_model(prompt, model=\"gemma3:4b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 5000\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    \n",
    "    request = req.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with req.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data\n",
    "\n",
    "\n",
    "def generate_model_scores(test_set, model_responses, model='gemma3:4b'):\n",
    "    scores = []\n",
    "    for i, entry in enumerate(test_set):\n",
    "        prompt = get_query(entry, model_response=model_responses[str(i)])\n",
    "        print(f\"Scoring example {i+1}\")\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score.strip().split()[0]))\n",
    "        except ValueError:\n",
    "            print(f\"⚠️ Could not convert score: {score}\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13a0d5fd-eca3-46f9-8bf2-d0adbc97b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring example 1\n",
      "Scoring example 2\n",
      "Scoring example 3\n",
      "Scoring example 4\n",
      "Scoring example 5\n",
      "Scoring example 6\n",
      "Scoring example 7\n",
      "Scoring example 8\n",
      "Scoring example 9\n",
      "Scoring example 10\n"
     ]
    }
   ],
   "source": [
    "with open(\"gemma_responses.json\", \"r\") as f:\n",
    "    gemma_data = json.load(f)\n",
    "\n",
    "gemma_scores = generate_model_scores(test_set, gemma_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69a27c7f-964d-4f3a-b337-2dced263a451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 100, 100, 0, 100, 100, 100, 100, 40]\n"
     ]
    }
   ],
   "source": [
    "print(gemma_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12adc8f2-f20a-445d-9424-17238e146e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring example 1\n",
      "Scoring example 2\n",
      "Scoring example 3\n",
      "Scoring example 4\n",
      "Scoring example 5\n",
      "Scoring example 6\n",
      "Scoring example 7\n",
      "Scoring example 8\n",
      "Scoring example 9\n",
      "Scoring example 10\n"
     ]
    }
   ],
   "source": [
    "with open(\"phi_responses.json\", \"r\") as f:\n",
    "    phi_data = json.load(f)\n",
    "\n",
    "phi_scores = generate_model_scores(test_set, phi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e58cd91-feb6-4642-af31-09f8d58e2fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 80, 0, 100, 0, 100, 90, 30, 0, 20]\n"
     ]
    }
   ],
   "source": [
    "print(phi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55bebf5d-4042-4179-bf9e-2e118e5d0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring example 1\n",
      "Scoring example 2\n",
      "Scoring example 3\n",
      "Scoring example 4\n",
      "Scoring example 5\n",
      "Scoring example 6\n",
      "Scoring example 7\n",
      "Scoring example 8\n",
      "Scoring example 9\n",
      "Scoring example 10\n"
     ]
    }
   ],
   "source": [
    "with open(\"mistral_responses.json\", \"r\") as f:\n",
    "    mistral_data = json.load(f)\n",
    "\n",
    "mistral_scores = generate_model_scores(test_set, mistral_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "157454eb-7b7e-4c58-bd1b-bca00ea38a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 60, 100, 100, 100, 100, 100, 30, 95, 20]\n"
     ]
    }
   ],
   "source": [
    "print(mistral_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b818b6f-39f9-4930-a250-5d3c8eac1460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma average score: 84.0\n",
      "Phi average score: 52.0\n",
      "Mistral average score: 80.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gemma average score: {sum(gemma_scores) / 10}\")\n",
    "print(f\"Phi average score: {sum(phi_scores) / 10}\")\n",
    "print(f\"Mistral average score: {sum(mistral_scores) / 10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
