{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858fb864-6006-4926-b11d-a606b43a21a5",
   "metadata": {},
   "source": [
    "# Define Variables, Parameters, and Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6c666a-0e87-4f37-94c8-cc920173efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as: sdhaduk\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "hf_token = \"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\"\n",
    "\n",
    "login(token=hf_token)\n",
    "user_info = whoami()\n",
    "print(\"Logged in as:\", user_info[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339ef361-3345-40dc-b792-ed1fc2decf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, logging\n",
    "from peft import LoraConfig, PeftModel\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_one_name = \"google/gemma-2b-it\" # already instruction fine-tuned\n",
    "model_two_name = \"microsoft/phi-2\" # focused  on code, chat and QA tasks\n",
    "model_three_name = \"mistralai/Mistral-7B-v0.1\" # only pretrained, but much larger (7B params)\n",
    "\n",
    "# LoRA parameters\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# bistandbytes parameters\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "# training argument params\n",
    "output_dir = \"./results\"\n",
    "logging_dir = \"./logs\"\n",
    "epochs = 1\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 2\n",
    "max_grad_norm = 0.3\n",
    "gradient_accumulation_steps = 1\n",
    "lr = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "log_steps = 50\n",
    "eval_steps = 500\n",
    "\n",
    "# SFT params\n",
    "max_seq_len = 256\n",
    "packing = True\n",
    "device_map = {\"\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e334f42-7e2b-489e-a770-ec285a6d439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4357a6-db49-41d5-a7d3-ce6f2fc9b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=logging_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=log_steps,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    max_seq_length=max_seq_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328bfd3d-b4e3-4be5-9a38-14c5c585621f",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0b4c7e5-d311-4ba0-956c-1892b5a1b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Clinton/Text-to-sql-v1\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "dataset = dataset.select(range(25000))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03d3783-c05f-4fc8-bcee-7b742dd84b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Name the home team for carlton away team', 'input': 'CREATE TABLE table_name_77 (\\n    home_team VARCHAR,\\n    away_team VARCHAR\\n)', 'response': 'SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"', 'source': 'sql_create_context', 'text': 'Below are sql tables schemas paired with instruction that describes a task. Using valid SQLite, write a response that appropriately completes the request for the provided tables. ### Instruction: Name the home team for carlton away team ### Input: CREATE TABLE table_name_77 (\\n    home_team VARCHAR,\\n    away_team VARCHAR\\n) ### Response: SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"'}\n"
     ]
    }
   ],
   "source": [
    "train_set = dataset.select(range(0, 20000))        # First 20,000 for training\n",
    "test_set = dataset.select(range(20000, 24000))     # Next 4,000 for testing\n",
    "val_set = dataset.select(range(24000, 25000))      # Last 1,000 for validation\n",
    "\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ec9c9-2174-4128-9985-0aa52fc61038",
   "metadata": {},
   "source": [
    "# Load and Fine-tune Each Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de20f8-543b-4079-b38b-2872ab1c67b2",
   "metadata": {},
   "source": [
    "## Gemma-2B-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "398695e6-d9ec-476d-b75f-41e43962d227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812048f58493493bb5c0c1d55395347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_one_name,\n",
    "    token=hf_token,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "gemma_model.config.pretraining_tp = 1\n",
    "gemma_model.config.use_cache = False\n",
    "\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_one_name,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "gemma_tokenizer.pad_token = gemma_tokenizer.eos_token\n",
    "gemma_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43c9c02e-9970-4c9d-a301-d35d9c9df84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): GemmaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ") \n",
      "\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(gemma_model,\"\\n\")\n",
    "print(next(gemma_model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a3226af-b590-41a4-a418-e1e13a7a0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "139bdab1-7910-4a95-84cc-86ac0be943e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "Name the home team for carlton away team\n",
      "\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "def format_for_gemma_it(example):\n",
    "    return {\n",
    "        \"text\": f\"<start_of_turn>user\\n{example['instruction']}\\n\\n{example['input']}\\n<end_of_turn>\\n\"\n",
    "                f\"<start_of_turn>model\\n{example['response']}\\n<end_of_turn>\"\n",
    "    }\n",
    "\n",
    "gemma_train_set = train_set.map(format_for_gemma_it, remove_columns=dataset.column_names)\n",
    "gemma_test_set = test_set.map(format_for_gemma_it, remove_columns=dataset.column_names)\n",
    "gemma_val_set = val_set.map(format_for_gemma_it, remove_columns=dataset.column_names)\n",
    "\n",
    "print(gemma_train_set[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ee5b5cb-387c-4f42-b269-09df04208ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    return example[\"text\"]\n",
    "    \n",
    "trainer = SFTTrainer(\n",
    "    model=gemma_model,\n",
    "    args=training_args,\n",
    "    train_dataset=gemma_train_set,\n",
    "    eval_dataset=gemma_val_set,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    processing_class=gemma_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce852f50-3302-41d2-aa8c-37aeaaac3def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample:\n",
      " <start_of_turn>user\n",
      "Name the home team for carlton away team\n",
      "\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"\n",
      "<end_of_turn> \n",
      "\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "Name the home team for carlton away team\n",
      "\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Sample:\\n\",gemma_train_set[0][\"text\"],\"\\n\\n\")\n",
    "input_text = \"<start_of_turn>user\\nName the home team for carlton away team\\n\\nCREATE TABLE table_name_77 (\\n    home_team VARCHAR,\\n    away_team VARCHAR\\n)\\n<end_of_turn>\"\n",
    "\n",
    "input_ids = gemma_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = gemma_model.generate(**input_ids, max_new_tokens=50)\n",
    "print(gemma_tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80fb3bec-e8e6-4b62-94d6-ea2715601a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 45:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.751500</td>\n",
       "      <td>0.778836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.694600</td>\n",
       "      <td>0.687183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>0.655601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.634724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.616568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.655200</td>\n",
       "      <td>0.610935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.639300</td>\n",
       "      <td>0.599228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.596571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.590211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.586613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(f\"{output_dir}/{model_one_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8f02de-0f8c-425d-817e-c02f4241b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample:\n",
      " <start_of_turn>user\n",
      "Name the home team for carlton away team\n",
      "\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"\n",
      "<end_of_turn> \n",
      "\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "Name the home team for carlton away team\n",
      "\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Sample:\\n\",gemma_train_set[0][\"text\"],\"\\n\\n\")\n",
    "input_text = \"<start_of_turn>user\\nName the home team for carlton away team\\n\\nCREATE TABLE table_name_77 (\\n    home_team VARCHAR,\\n    away_team VARCHAR\\n)\\n<end_of_turn>\"\n",
    "\n",
    "input_ids = gemma_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = gemma_model.generate(**input_ids, max_new_tokens=50)\n",
    "print(gemma_tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b2062-53a2-4a9e-aeee-efd141085efb",
   "metadata": {},
   "source": [
    "## Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb8711f-b150-4739-a9b9-9f789ebfdc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49b1772b3ee48648e5ae13097abe38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_two_name,\n",
    "    token=hf_token,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "phi_model.config.pretraining_tp = 1\n",
    "phi_model.config.use_cache = False\n",
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_two_name,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f82fc5e-d218-47b4-9f69-af0544917f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiAttention(\n",
      "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (rotary_emb): PhiRotaryEmbedding()\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ") \n",
      "\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(phi_model,\"\\n\")\n",
    "print(next(phi_model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e97a316b-1c71-4fcf-b759-ba488d174f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5c9b5f4-4926-4742-843a-2407ff7d481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43993, 25, 6530, 262, 1363, 1074, 329, 1097, 75, 1122, 1497, 1074, 198, 20560, 25, 29244, 6158, 43679, 3084, 62, 3672, 62, 3324, 357, 198, 50284, 11195, 62, 15097, 569, 31315, 1503, 11, 198, 50284, 8272, 62, 15097, 569, 31315, 1503, 198, 8, 198, 26410, 25, 33493, 1363, 62, 15097, 16034, 3084, 62, 3672, 62, 3324, 33411, 1497, 62, 15097, 796, 366, 66, 7063, 1122, 1]\n"
     ]
    }
   ],
   "source": [
    "MAX_CONTEXT_LEN = 2048\n",
    "\n",
    "def format_and_tokenize_for_phi2(example):\n",
    "    text = f\"Instruct: {example['instruction']}\\nInput: {example['input']}\\nOutput: {example['response']}\"\n",
    "    tokenized = phi_tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_CONTEXT_LEN,\n",
    "        padding=False,  \n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "phi2_train_set = train_set.map(format_and_tokenize_for_phi2, remove_columns=train_set.column_names)\n",
    "phi2_val_set = val_set.map(format_and_tokenize_for_phi2, remove_columns=val_set.column_names)\n",
    "phi2_test_set = test_set.map(format_and_tokenize_for_phi2, remove_columns=test_set.column_names)\n",
    "\n",
    "print(phi2_train_set[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "678e312b-b718-40ef-9784-78522c5096cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=phi_model,\n",
    "    args=training_args,\n",
    "    train_dataset=phi2_train_set,\n",
    "    eval_dataset=phi2_val_set,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=phi_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e66174a-5dba-4130-8191-3c3d3d07900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_30788\\1308842867.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.bfloat16):\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Name the home team for carlton away team\n",
      "Input: CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "Output: CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR(255),\n",
      "    away_team VARCHAR(255)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Construct input\n",
    "instruction = \"Name the home team for carlton away team\"\n",
    "input_sql = \"CREATE TABLE table_name_77 (\\n    home_team VARCHAR,\\n    away_team VARCHAR\\n)\"\n",
    "input_text = f\"Instruct: {instruction}\\nInput: {input_sql}\\nOutput:\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = phi_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate with AMP autocast\n",
    "with autocast(dtype=torch.bfloat16):\n",
    "    outputs = phi_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "print(phi_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51fdbdc3-09fa-4c0e-a83b-2ceef983cf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 1:17:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.099100</td>\n",
       "      <td>1.130019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.956167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.923760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.883200</td>\n",
       "      <td>0.817331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>0.778143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.747285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>0.723924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.710765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>0.697268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>0.686854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(f\"{output_dir}/{model_two_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4536755-77ca-43c2-907a-05bc66e2c38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_30788\\396117594.py:2: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.bfloat16):\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Name the home team for carlton away team\n",
      "Input: CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "Output: SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\" AND home_team = \"carlton\"\n",
      "Output: SELECT home_team FROM table_name_77 WHERE away_team = \"c\n"
     ]
    }
   ],
   "source": [
    "# Generate with AMP autocast\n",
    "with autocast(dtype=torch.bfloat16):\n",
    "    outputs = phi_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "print(phi_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20011f9a-b178-4d00-b8d3-96e01e2baed4",
   "metadata": {},
   "source": [
    "## Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f37ac5-48d7-4dfa-935a-7006101918b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf82fdf2a9645509512174318df3e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321a69ea35064c2d922e3dee0d25fb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\VSCode\\finetuning-experiment\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sagar\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80a214033e84630ac1e6d7081425d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fab34032d07426ab59ac0ca81cd7410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89a7045d1a2481686c621b8764574a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f642eee9184b22ad6fd65a9dfbc156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_three_name,\n",
    "    token=hf_token,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "mistral_model.config.pretraining_tp = 1\n",
    "mistral_model.config.use_cache = False\n",
    "\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_three_name,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70932023-da3d-4d6a-89b2-2ff81bee7890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ") \n",
      "\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(mistral_model,\"\\n\")\n",
    "print(next(mistral_model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298b569d-e619-41c1-9786-1d4381713015",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[ \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cab75b9-500c-4fbc-a19f-28a61181e2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Name the home team for carlton away team\n",
      "\n",
      "### Input:\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "\n",
      "### Response:\n",
      "SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"\n"
     ]
    }
   ],
   "source": [
    "def format_for_mistral_basic(example):\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = example[\"input\"].strip()\n",
    "    response = example[\"response\"].strip()\n",
    "\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{response}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "\n",
    "mistral_train_set = train_set.map(format_for_mistral_basic, remove_columns=train_set.column_names)\n",
    "mistral_train_set = mistral_train_set.select(range(5000))\n",
    "\n",
    "mistral_test_set = test_set.map(format_for_mistral_basic, remove_columns=test_set.column_names)\n",
    "mistral_val_set = val_set.map(format_for_mistral_basic, remove_columns=val_set.column_names)\n",
    "mistral_val_set = mistral_val_set.select(range(100))\n",
    "\n",
    "print(mistral_train_set[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f239b0fb-d2bb-4f16-8551-6264fef39300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(mistral_train_set))\n",
    "print(len(mistral_val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c96b036f-2fa2-43cf-9665-6dbaac88cf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30984eacf20e41dfad5b0daf1d418f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9868c1ea235f4db1bcb0aab05f8e56dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b94226d43b749e5836fe3cbe198239e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e901e77388d44aa9bce73f0d62d19549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7296a93edb5b49a5875190e52771a6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0783298194eb48558828fb021d747051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1bbd0a4824db58ec6194a4ee15adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1ef2c3a1c14a96b4e69575ac2db063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f0307c1af44f7e9e63d7402f6c808c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003bbf2198b44025b3c7f09ad907e98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    return example[\"text\"]\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=mistral_model,\n",
    "    args=training_args,\n",
    "    train_dataset=mistral_train_set,\n",
    "    eval_dataset=mistral_val_set,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=mistral_tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "125e4e33-275d-45e9-8681-3b6fdad76361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample:\n",
      " ### Instruction:\n",
      "Name the home team for carlton away team\n",
      "\n",
      "### Input:\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "\n",
      "### Response:\n",
      "SELECT home_team FROM table_name_77 WHERE away_team = \"carlton\" \n",
      "\n",
      "\n",
      "Model Output:\n",
      " ### Instruction:\n",
      "Name the home team for carlton away team\n",
      "\n",
      "### Input:\n",
      "CREATE TABLE table_name_77 (\n",
      "    home_team VARCHAR,\n",
      "    away_team VARCHAR\n",
      ")\n",
      "\n",
      "### Response:\n",
      "\n",
      "INSERT INTO table_name_77 VALUES ('Melbourne', 'Carlton')\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "This is the sixth entry in a sequence of seven questions in the same database.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "from torch import autocast \n",
    "\n",
    "# Print one training sample\n",
    "print(\"Training Sample:\\n\", mistral_train_set[0][\"text\"], \"\\n\\n\")\n",
    "\n",
    "# Example input\n",
    "instruction = \"Name the home team for carlton away team\"\n",
    "input_text = \"\"\"CREATE TABLE table_name_77 (\n",
    "    home_team VARCHAR,\n",
    "    away_team VARCHAR\n",
    ")\"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = mistral_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):  # or torch.float16 if your model uses fp16\n",
    "        outputs = mistral_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=mistral_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "# Decode\n",
    "response = mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Model Output:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f0ef7c9-fd2a-4f61-a411-7e3c6b5c003a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 2:16:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>0.638401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>0.576986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(f\"{output_dir}/{model_three_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
